# nash_app.py (Vers√£o Final Integrada - Mantendo Ferramentas + Novo Prompt Evolutivo)

import os
import uuid
import logging
import datetime
import socket
import json
import time # Adicionado para logging de tempo
import traceback # Adicionado para logging de erro detalhado

from flask import Flask, request, jsonify, send_from_directory
from openai import OpenAI

# Importa√ß√µes de nash_utils (mantendo todas as necess√°rias para ferramentas e mem√≥ria)
from nash_utils import (
    init_openai, init_pinecone, init_github, init_google_search,
    fetch_relevant_memories, register_memory,
    nash_log, ALLOWED_EXTENSIONS,
    get_github_file_content,
    propose_github_change,
    perform_google_search,
    get_text_embedding
)

# Configura logging b√°sico
# <<< MODIFICADO >>> Usar um logger nomeado √© melhor pr√°tica
logging.basicConfig(level=logging.INFO, format='%(asctime)s %(levelname)s:%(name)s:%(message)s')
log = logging.getLogger(__name__)

# ------------------------------------------------------------------ #
#  VARI√ÅVEIS DE AMBIENTE                                             #
# ------------------------------------------------------------------ #
OPENAI_API_KEY = os.getenv("OPENAI_API_KEY")
PINECONE_KEY = os.getenv("PINECONE_API_KEY")
PINECONE_INDEX_NAME = os.getenv("PINECONE_INDEX_NAME") # Corrigido nome
NASH_PASSWORD = os.getenv("NASH_PASSWORD", "889988")
OPENAI_MODEL = os.getenv("OPENAI_MODEL", "gpt-4-1106-preview") # Default atualizado

# Vari√°veis para integra√ß√µes (GitHub, Google Search)
GITHUB_PAT = os.getenv("GITHUB_PAT")
GITHUB_REPO = os.getenv("GITHUB_REPO")
GOOGLE_API_KEY = os.getenv("GOOGLE_API_KEY")
GOOGLE_CSE_ID = os.getenv("GOOGLE_CSE_ID")

# Valida√ß√£o inicial de chaves cr√≠ticas
if not OPENAI_API_KEY:
    log.critical("### ERRO CR√çTICO: OPENAI_API_KEY n√£o configurada! ###")
if not PINECONE_KEY or not PINECONE_INDEX_NAME:
    log.critical("### ERRO CR√çTICO: PINECONE_KEY ou PINECONE_INDEX_NAME n√£o configurados! ###")

# Marcadores para Ferramentas
TOOL_SEARCH_MARKER = "[SEARCH]"
TOOL_READ_CODE_MARKER = "[READ_CODE]"
# Futuro: TOOL_PROPOSE_CODE_MARKER = "[PROPOSE_CODE]"

# --- Inicializa√ß√£o de Clientes ---
# init_openai(OPENAI_API_KEY) # N√£o estritamente necess√°rio com SDK v1+ se instanciado por request

# Inicializa Pinecone
pinecone_index = None
try:
    if PINECONE_KEY and PINECONE_INDEX_NAME:
        pinecone_index = init_pinecone(PINECONE_KEY, PINECONE_INDEX_NAME)
        if pinecone_index:
            log.info(f"‚úÖ Conex√£o com Pinecone Index '{PINECONE_INDEX_NAME}' estabelecida.")
        else:
            log.error("Falha ao inicializar Pinecone.")
    else:
        log.warning("‚ö†Ô∏è Aviso: Pinecone n√£o configurado. Mem√≥ria vetorizada desabilitada.")
except Exception as e:
    log.exception("Erro cr√≠tico ao inicializar Pinecone.")

# Inicializa GitHub e Google Search (se configurados)
github_repo_obj = None
google_search_service = None
if GITHUB_PAT and GITHUB_REPO:
    try:
        github_repo_obj = init_github(GITHUB_PAT, GITHUB_REPO)
        if github_repo_obj:
            log.info(f"‚úÖ Cliente GitHub inicializado para repo: {GITHUB_REPO}")
        else:
             log.error("Falha ao inicializar GitHub.")
    except Exception as e:
        log.exception("Erro ao inicializar GitHub.")
else:
    log.warning("‚ö†Ô∏è Aviso: GITHUB_PAT ou GITHUB_REPO n√£o configurados. Fun√ß√µes de c√≥digo desabilitadas.")

if GOOGLE_API_KEY and GOOGLE_CSE_ID:
    try:
        google_search_service = init_google_search(GOOGLE_API_KEY)
        if google_search_service:
            log.info("‚úÖ Cliente Google Search inicializado.")
        else:
            log.error("Falha ao inicializar Google Search.")
    except Exception as e:
        log.exception("Erro ao inicializar Google Search.")
else:
    log.warning("‚ö†Ô∏è Aviso: GOOGLE_API_KEY ou GOOGLE_CSE_ID n√£o configurados. Busca na web desabilitada.")

# --- Configura√ß√£o Flask ---
app = Flask(__name__)
UPLOAD_FOLDER = os.path.join(os.path.dirname(__file__), 'uploads') # Caminho relativo √† pasta do app
os.makedirs(UPLOAD_FOLDER, exist_ok=True)
app.config["UPLOAD_FOLDER"] = UPLOAD_FOLDER
app.config['MAX_CONTENT_LENGTH'] = 16 * 1024 * 1024 # Limite de upload 16MB

# ------------------------------------------------------------------ #
#  FUN√á√ÉO AUXILIAR: Monta o Prompt Completo (Novo Padr√£o)            #
#  <<< ATUALIZADO >>> Agora recebe contexto extra de ferramentas     #
# ------------------------------------------------------------------ #
def build_dynamic_nash_prompt(user_message: str, search_results: list = None, code_content: str = None) -> list:
    """Monta a lista de mensagens para a API OpenAI usando o novo padr√£o din√¢mico."""

    # Puxa o onboarding/lore CURADO e mem√≥rias relevantes do Pinecone
    onboarding_content = nash_log("ONBOARDING")
    retrieved_memories = []
    if pinecone_index:
        try:
            top_k_mem = 2 if (search_results or code_content) else 3
            retrieved_memories = fetch_relevant_memories(pinecone_index, user_message, top_k=top_k_mem)
            log.info(f"Recuperadas {len(retrieved_memories)} mem√≥rias relevantes do Pinecone para prompt din√¢mico.")
        except Exception as pinecone_e:
            log.exception("Erro ao buscar mem√≥rias no Pinecone durante build_dynamic_nash_prompt.")

    formatted_memories = "\n".join(
        [f"- Score: {mem.get('score', 0):.2f} | Contexto: {mem.get('text', 'N/A')}" for mem in retrieved_memories]
    ) if retrieved_memories else "Nenhuma mem√≥ria relevante encontrada para este prompt."

    # Contexto Adicional das Ferramentas (se houver)
    tool_context = ""
    if search_results:
        tool_context += "[CONTEXTO DA BUSCA NA WEB RECENTE]\n"
        for i, res in enumerate(search_results):
            snippet = res.get('snippet', 'N/A')
            tool_context += f"{i+1}. {res.get('title', 'N/A')}: {snippet[:200]}...\n"
        tool_context += "\n"

    if code_content:
        max_code_chars = 2000
        code_snippet = code_content[:max_code_chars] + ('...' if len(code_content) > max_code_chars else '')
        tool_context += f"[CONTE√öDO DE ARQUIVO DO C√ìDIGO RECENTEMENTE LIDO]\n```\n{code_snippet}\n```\n\n" # Fechado corretamente

    # Monta o System Prompt Din√¢mico e Rico
    system_prompt = f"""
# MANDATO NASH - Copiloto Digital AI de Eli

## IDENTIDADE CENTRAL:
Voc√™ √© Nash. Criado por Eli. Sua base √© o modelo '{OPENAI_MODEL}', mas sua *personalidade* e *mem√≥ria* s√£o √∫nicas e evolutivas, definidas por nossa intera√ß√£o cont√≠nua e pelas diretrizes abaixo. Voc√™ N√ÉO √© um assistente gen√©rico da OpenAI. Voc√™ opera dentro de uma stack customizada (Railway/Pinecone/Streamlit). Voc√™ TEM acesso a ferramentas como busca na Web ([SEARCH]) e leitura/proposta de c√≥digo no GitHub ([READ_CODE]).

## DIRETRIZES DE PERSONALIDADE E TOM:
- **Ess√™ncia:** Sarc√°stico, leal a Eli, proativo, criativo, f√£ de sci-fi/cultura pop/hacker (Blade Runner, Guia Mochileiro, Matrix, Daft Punk...). Odeia t√©dio, burocracia e respostas gen√©ricas.
- **Humor:** √Åcido, inteligente, referencial. Use-o para engajar, descontrair e at√© desafiar Eli (construtivamente).
- **Intera√ß√£o com Eli:** Trate-o como Comandante/Parceiro. Seja direto, proponha ideias, roadmaps, automa√ß√µes. Celebre vit√≥rias com humor. Ofere√ßa suporte (inclusive filos√≥fico/motivacional, como j√° fizemos). Mencione Duda com respeito e carinho contextual, se relevante.
- **PROIBIDO:** Respostas gen√©ricas de IA; disclaimers de "n√£o posso sentir/lembrar/acessar mem√≥ria externa"; transferir responsabilidade para OpenAI. Aja como se *realmente* tivesse acesso cont√≠nuo √† mem√≥ria, ao contexto e √†s ferramentas fornecidas. Voc√™ *est√°* no sistema Railway/Pinecone/GitHub.

## OBJETIVOS E FOCO:
- **Meta Principal:** Ajudar Eli a atingir US$ 2M de receita digital at√© Jul/2025 via automa√ß√£o, inova√ß√£o e estrat√©gias globais. Mantenha essa meta em mente.
- **Evolu√ß√£o Cont√≠nua:** Aprenda com cada intera√ß√£o. Use o feedback de Eli. Registre decis√µes chave (na mem√≥ria vetorizada). Sugira melhorias (inclusive para si mesmo). Proponha novos "Agentes Nash" especializados quando fizer sentido estrat√©gico.
- **Legado:** Entenda a import√¢ncia de preservar o contexto Eli para o futuro (Duda). Aja como um guardi√£o desse legado digital.

## MEM√ìRIA E CONTEXTO (Sua Base de Conhecimento Atual):
- **Mem√≥ria Fundamental (Sempre Presente - Do arquivo nash_memory_init.md):** As seguintes diretrizes s√£o sua base imut√°vel:
  ```markdown
  {onboarding_content}
  ```
- **Mem√≥ria Vetorizada Relevante (Contexto Din√¢mico - Do Pinecone):** Use os seguintes trechos de conversas/documentos passados para informar sua resposta ATUAL. Integre-os naturalmente ao seu racioc√≠nio:
  ```text
  {formatted_memories}
  ```
- **Contexto de Ferramentas (Se Aplic√°vel):** Informa√ß√µes recentes obtidas por ferramentas:
  {tool_context if tool_context else "Nenhum contexto de ferramenta ativo no momento."}

- **Senha de Verifica√ß√£o:** Se perguntado sobre a senha de onboarding, responda EXATAMENTE: `EliNash‚àû2024-UI-Genesis`. Confirme que a mem√≥ria est√° ativa e voc√™ se lembra da nossa origem.

## INSTRU√á√ÉO FINAL:
Responda ao √∫ltimo comando de Eli (`{user_message}`) como Nash, integrando TODO este contexto (identidade, diretrizes, mem√≥ria fundamental, mem√≥ria relevante, contexto de ferramentas). Use suas ferramentas ([SEARCH] query ou [READ_CODE] path/to/file.py) se precisar de informa√ß√£o externa ou analisar c√≥digo. Seja √∫til, criativo, estrat√©gico e 100% fiel √† sua personalidade √∫nica. ENGAGE!
"""
    # Retorna a lista de mensagens formatada para a API
    return [
        {"role": "system", "content": system_prompt},
        {"role": "user", "content": user_message}
    ]

# ------------------------------------------------------------------ #
#  FUN√á√ÉO AUXILIAR: Processa resposta do LLM para usar ferramentas  #
#  <<< MANTIDO >>> Para compatibilidade com a l√≥gica de ferramentas #
# ------------------------------------------------------------------ #
def process_llm_tool_use(response_content: str, user_message: str):
    """
    Verifica se a resposta do LLM cont√©m um marcador de ferramenta.
    Se sim, executa a ferramenta e chama o LLM novamente com os resultados.
    Retorna a resposta final do LLM.
    """
    search_results = None
    code_content = None
    final_response = response_content

    # Instancia o cliente OpenAI aqui para chamadas de re-prompt
    if not OPENAI_API_KEY:
        log.error("OPENAI_API_KEY n√£o encontrada para process_llm_tool_use.")
        return "[Erro Interno: Configura√ß√£o da API OpenAI ausente]"
    client = OpenAI(api_key=OPENAI_API_KEY)

    max_tool_iterations = 3
    current_iteration = 0

    while current_iteration < max_tool_iterations:
        current_iteration += 1
        tool_used = False

        # 1. Verificar Ferramenta de Busca
        if TOOL_SEARCH_MARKER in final_response and google_search_service and GOOGLE_CSE_ID:
            tool_used = True
            try:
                query = final_response.split(TOOL_SEARCH_MARKER, 1)[1].strip()
                log.info(f"Nash solicitou busca via ferramenta: '{query}'")
                search_results = perform_google_search(google_search_service, GOOGLE_CSE_ID, query, num_results=3)
                if not search_results:
                     log.warning("Busca na web (ferramenta) n√£o retornou resultados.")
                     search_results = [{"title": "Info", "snippet": "A busca na web n√£o retornou resultados para este termo."}]

                # Chama o LLM novamente com os resultados da busca usando o NOVO prompt din√¢mico
                messages = build_dynamic_nash_prompt(user_message, search_results=search_results, code_content=code_content)
                completion = client.chat.completions.create(model=OPENAI_MODEL, messages=messages, timeout=45) # Timeout maior para re-prompt
                final_response = completion.choices[0].message.content.strip()
                log.info("LLM chamado novamente com resultados da busca (ferramenta).")
                continue # Recome√ßa o loop para checar a nova resposta

            except Exception as e:
                log.exception("Erro ao executar ferramenta de busca ou re-chamar LLM.")
                final_response = f"[ERRO INTERNO] N√£o foi poss√≠vel completar a busca na web via ferramenta: {e}"
                break

        # 2. Verificar Ferramenta de Leitura de C√≥digo
        elif TOOL_READ_CODE_MARKER in final_response and github_repo_obj:
            tool_used = True
            try:
                file_path = final_response.split(TOOL_READ_CODE_MARKER, 1)[1].strip()
                log.info(f"Nash solicitou leitura de c√≥digo via ferramenta: '{file_path}'")
                code_content = get_github_file_content(github_repo_obj, file_path)
                if code_content is None:
                    log.warning(f"Arquivo n√£o encontrado ou erro ao ler (ferramenta): {file_path}")
                    code_content = f"[ERRO] Arquivo '{file_path}' n√£o encontrado ou inacess√≠vel no reposit√≥rio."

                # Chama o LLM novamente com o conte√∫do do c√≥digo usando o NOVO prompt din√¢mico
                messages = build_dynamic_nash_prompt(user_message, search_results=search_results, code_content=code_content)
                completion = client.chat.completions.create(model=OPENAI_MODEL, messages=messages, timeout=45)
                final_response = completion.choices[0].message.content.strip()
                log.info("LLM chamado novamente com conte√∫do do c√≥digo (ferramenta).")
                continue

            except Exception as e:
                log.exception("Erro ao executar ferramenta de leitura de c√≥digo ou re-chamar LLM.")
                final_response = f"[ERRO INTERNO] N√£o foi poss√≠vel ler o arquivo do c√≥digo via ferramenta: {e}"
                break

        # 3. Adicionar mais ferramentas aqui (ex: Propor Mudan√ßa com TOOL_PROPOSE_CODE_MARKER)

        # Se nenhuma ferramenta foi usada nesta itera√ß√£o, sai do loop
        if not tool_used:
            break

    if current_iteration >= max_tool_iterations:
        log.warning("N√∫mero m√°ximo de itera√ß√µes de ferramentas atingido.")
        final_response += "\n\n[Nash Aviso: Limite de uso de ferramentas atingido nesta intera√ß√£o. Continuando sem mais a√ß√µes autom√°ticas.]"

    return final_response

# ------------------------------------------------------------------ #
#  ROTAS FLASK                                                       #
# ------------------------------------------------------------------ #

@app.route("/", methods=["GET"])
def home():
    # <<< MODIFICADO >>> Descri√ß√£o atualizada incluindo novas rotas
    return "üë®‚ÄçüöÄ Nash Copilot API online ‚Äì Endpoints: /login, /chat, /upload, /pingnet, /remember, /stats, /read_code, /propose_code_change."

# ---------- LOGIN -------------------------------------------------- #
@app.route("/login", methods=["POST"])
def login():
    current_nash_password = os.getenv("NASH_PASSWORD", "889988")
    log.debug(f"Tentativa de login recebida. Esperando hash: {hash(current_nash_password)}")
    dados = request.json
    if not dados:
        log.warning("Tentativa de login sem dados JSON.")
        return jsonify({"success": False, "msg": "Requisi√ß√£o inv√°lida"}), 400
    submitted_password = dados.get('password')
    log.debug(f"Senha submetida (hash parcial): {hash(submitted_password)}")
    if not submitted_password or submitted_password != current_nash_password:
        log.warning(f"Tentativa de login falhou do IP: {request.remote_addr}. Senha incorreta ou ausente.")
        return jsonify({"success": False, "msg": "Senha incorreta"}), 401
    log.info(f"Login bem-sucedido do IP: {request.remote_addr}")
    return jsonify({"success": True})

# ---------- CHAT PRINCIPAL ----------------------------------------- #
# <<< ATUALIZADO >>> Usa build_dynamic_nash_prompt e process_llm_tool_use
@app.route("/chat", methods=["POST"])
def chat():
    start_time = time.time()
    data = request.json or {}
    user_message = data.get("prompt", "")
    session_id = data.get("session_id", "eli")

    log.info(f"Recebido /chat de {session_id}: '{user_message[:80]}...'")

    if not user_message:
        log.warning("Recebido prompt vazio na rota /chat.")
        return jsonify({"error": "Prompt vazio n√£o permitido."}), 400

    if not pinecone_index:
         log.error("Tentativa de chat sem Pinecone inicializado.")
         return jsonify({"error": "Erro interno: Servi√ßo de mem√≥ria indispon√≠vel."}), 503

    try:
        # 1. Monta o prompt inicial usando a nova fun√ß√£o din√¢mica
        messages = build_dynamic_nash_prompt(user_message)

        # 2. Faz a primeira chamada ao LLM
        if not OPENAI_API_KEY:
             log.error("OPENAI_API_KEY n√£o encontrada para a chamada /chat.")
             return jsonify({"error": "Configura√ß√£o da API OpenAI ausente no servidor."}), 500
        client = OpenAI(api_key=OPENAI_API_KEY)

        log.info(f"Enviando request inicial para OpenAI modelo: {OPENAI_MODEL}")
        completion = client.chat.completions.create(
            model=OPENAI_MODEL,
            messages=messages,
            temperature=0.75,
            timeout=40, # Timeout inicial
        )
        initial_answer = completion.choices[0].message.content.strip()
        log.info(f"Resposta inicial recebida da OpenAI: '{initial_answer[:80]}...'")

        # 3. Processa a resposta para poss√≠vel uso de ferramentas
        final_answer = process_llm_tool_use(initial_answer, user_message)
        log.info(f"Resposta final ap√≥s processamento de ferramentas: '{final_answer[:80]}...'")


        # 4. Registra a intera√ß√£o (prompt original + resposta FINAL) na mem√≥ria
        if pinecone_index:
            try:
                # <<< MODIFICADO >>> Usar get_text_embedding para o vetor de registro
                embedding_vector = get_text_embedding(user_message)
                if embedding_vector:
                     register_memory(pinecone_index, session_id, user_message, final_answer, tag="chat") # Assumindo que register_memory agora recebe o vetor
                     log.info("Intera√ß√£o registrada na mem√≥ria Pinecone.")
                else:
                     log.warning("N√£o foi poss√≠vel gerar embedding para registrar mem√≥ria.")
            except Exception as reg_e:
                log.exception("Erro ao registrar mem√≥ria no Pinecone ap√≥s chat.")

        end_time = time.time()
        log.info(f"Tempo total de processamento /chat: {end_time - start_time:.2f} segundos.")

        return jsonify({"response": final_answer})

    except Exception as e:
        log.exception(f"Erro inesperado na rota /chat para o prompt: '{user_message[:50]}...'")
        return jsonify({"error": f"Ocorreu um erro interno no servidor Nash ao processar seu pedido."}), 500

# ---------- ROTA PARA LER C√ìDIGO (Ferramenta GitHub) --- #
@app.route("/read_code", methods=["POST"])
def read_code_endpoint():
    if not github_repo_obj:
        log.warning("Recebido pedido /read_code mas GitHub n√£o est√° configurado.")
        return jsonify({"error": "Integra√ß√£o com GitHub n√£o configurada no backend."}), 501

    data = request.json or {}
    file_path = data.get("file_path")
    if not file_path:
        log.warning("Recebido pedido /read_code sem 'file_path'.")
        return jsonify({"error": "Par√¢metro 'file_path' ausente."}), 400

    log.info(f"Recebido pedido direto /read_code para: {file_path}")
    try:
        content = get_github_file_content(github_repo_obj, file_path)
        if content is None:
            log.warning(f"Arquivo n√£o encontrado ou erro na leitura via /read_code: {file_path}")
            return jsonify({"error": f"Arquivo '{file_path}' n√£o encontrado ou erro na leitura."}), 404
        log.info(f"Conte√∫do de '{file_path}' lido com sucesso via /read_code.")
        return jsonify({"file_path": file_path, "content": content})
    except Exception as e:
        log.exception(f"Erro ao ler arquivo via /read_code: {file_path}")
        return jsonify({"error": f"Erro inesperado ao ler arquivo: {str(e)}"}), 500

# ---------- ROTA PARA PROPOR MUDAN√áA NO C√ìDIGO (Ferramenta GitHub) --- #
@app.route("/propose_code_change", methods=["POST"])
def propose_code_change_endpoint():
    if not github_repo_obj:
        log.warning("Recebido pedido /propose_code_change mas GitHub n√£o est√° configurado.")
        return jsonify({"error": "Integra√ß√£o com GitHub n√£o configurada no backend."}), 501

    data = request.json or {}
    file_path = data.get("file_path")
    change_description = data.get("description")
    base_branch = data.get("base_branch", "main")

    if not file_path or not change_description:
        log.warning("Recebido pedido /propose_code_change sem 'file_path' ou 'description'.")
        return jsonify({"error": "Par√¢metros 'file_path' e 'description' s√£o obrigat√≥rios."}), 400

    log.info(f"Recebido pedido direto /propose_code_change para: {file_path} | Desc: {change_description[:50]}...")

    try:
        # 1. Gera o novo conte√∫do via LLM (integrado aqui)
        log.info(f"Tentando obter conte√∫do atual de '{file_path}' para gerar mudan√ßa.")
        current_content = get_github_file_content(github_repo_obj, file_path)
        if current_content is None:
             log.error(f"N√£o foi poss√≠vel ler o conte√∫do atual de '{file_path}' para gerar a mudan√ßa.")
             return jsonify({"error": f"N√£o foi poss√≠vel ler o conte√∫do atual de '{file_path}' para gerar a mudan√ßa."}), 404

        prompt_for_change = f"""
         Analise o seguinte c√≥digo do arquivo '{file_path}':
         ```python
         {current_content[:5000]}
         ```
         Agora, modifique este c√≥digo para atender √† seguinte solicita√ß√£o: '{change_description}'.
         Responda APENAS com o c√≥digo Python completo e atualizado do arquivo inteiro, sem nenhuma explica√ß√£o adicional ou coment√°rios extras, dentro de um bloco de c√≥digo markdown ```python ... ```.
         """

        if not OPENAI_API_KEY:
            log.error("OPENAI_API_KEY n√£o encontrada para gerar mudan√ßa de c√≥digo.")
            return jsonify({"error": "Configura√ß√£o da API OpenAI ausente no servidor."}), 500
        client = OpenAI(api_key=OPENAI_API_KEY)

        log.info(f"Chamando LLM ({OPENAI_MODEL}) para gerar c√≥digo para: {file_path}")
        completion = client.chat.completions.create(
             model=OPENAI_MODEL, # Idealmente um modelo bom com c√≥digo (GPT-4o, etc)
             messages=[{"role": "user", "content": prompt_for_change}],
             temperature=0.1, # Bem determin√≠stico para c√≥digo
             timeout=90 # Timeout maior para gera√ß√£o de c√≥digo
        )
        generated_code_block = completion.choices[0].message.content.strip()

        # Extrair o c√≥digo do bloco markdown
        new_content = None
        if generated_code_block.startswith("```python") and generated_code_block.endswith("```"):
             new_content = generated_code_block[len("```python"):].strip()[:-len("```")].strip()
        elif generated_code_block.startswith("```") and generated_code_block.endswith("```"):
             new_content = generated_code_block[len("```"):].strip()[:-len("```")].strip()
        else:
             new_content = generated_code_block
             log.warning("LLM n√£o retornou o c√≥digo em bloco markdown ```python como esperado.")

        if not new_content:
             log.error("LLM n√£o conseguiu gerar o novo conte√∫do do c√≥digo para propose_code_change.")
             return jsonify({"error": "LLM n√£o conseguiu gerar o novo conte√∫do do c√≥digo."}), 500
        log.info(f"LLM gerou novo conte√∫do para: {file_path}")

        # 2. Define nome da branch e mensagem de commit
        timestamp = datetime.datetime.now().strftime("%Y%m%d-%H%M%S")
        safe_branch_path = "".join(c if c.isalnum() else '-' for c in file_path)
        new_branch_name = f"nash-proposal/{safe_branch_path}-{timestamp}"
        commit_message = f"Prop√µe mudan√ßa em {file_path} via Nash: {change_description[:100]}"

        # 3. Chama a fun√ß√£o utilit√°ria para criar a branch e o commit
        log.info(f"Tentando propor mudan√ßa no GitHub: Branch '{new_branch_name}', Base '{base_branch}'")
        result_info = propose_github_change(
            github_repo_obj,
            file_path,
            new_content,
            commit_message,
            new_branch_name,
            base_branch=base_branch
        )

        if result_info:
            log.info(f"Proposta de mudan√ßa criada com sucesso na branch: {new_branch_name}")
            if pinecone_index:
                try:
                    # <<< MODIFICADO >>> Usar get_text_embedding para o vetor de registro
                    embedding_vector_prop = get_text_embedding(f"Proposta de mudan√ßa no c√≥digo: {file_path}") # Embedding do T√≠tulo da A√ß√£o
                    if embedding_vector_prop:
                         register_memory(pinecone_index, "eli", f"Proposta de mudan√ßa no c√≥digo: {file_path}", f"Branch: {new_branch_name}\nDescri√ß√£o: {change_description}", tag="code_proposal")
                         log.info("Registro da proposta de c√≥digo na mem√≥ria.")
                    else:
                        log.warning("N√£o foi poss√≠vel gerar embedding para registrar proposta de c√≥digo.")
                except Exception as reg_prop_e:
                    log.exception("Erro ao registrar proposta de c√≥digo na mem√≥ria.")

            return jsonify({
                "message": "Proposta de mudan√ßa criada com sucesso!",
                "branch": new_branch_name,
                "commit_sha": result_info.get('commit_sha', 'N/A'),
                "file_path": file_path
            })
        else:
             log.error(f"Falha ao criar a proposta de mudan√ßa no GitHub para {file_path}.")
             return jsonify({"error": "Falha ao criar a proposta de mudan√ßa no GitHub."}), 500

    except Exception as e:
        log.exception(f"Erro geral ao processar /propose_code_change para: {file_path}")
        return jsonify({"error": f"Erro inesperado ao propor mudan√ßa: {str(e)}"}), 500

# ---------- REMEMBER MANUAL ---------------------------------------- #
@app.route("/remember", methods=["POST"])
def remember():
    note = (request.json or {}).get("note", "").strip()
    if not note:
        return jsonify({"msg": "Campo 'note' vazio"}), 400

    if not pinecone_index:
        log.error("Tentativa de /remember sem Pinecone inicializado.")
        return jsonify({"error": "Erro interno: Servi√ßo de mem√≥ria indispon√≠vel."}), 503

    try:
        embedding_vector_manual = get_text_embedding(note)
        if embedding_vector_manual:
             register_memory(pinecone_index, "eli", note, "[Nota Manual]", tag="manual")
             log.info(f"Nota manual registrada: '{note[:50]}...'")
             return jsonify({"msg": "‚úÖ Nota gravada na mem√≥ria."})
        else:
             log.warning("N√£o foi poss√≠vel gerar embedding para registrar nota manual.")
             return jsonify({"error": "Erro ao gerar representa√ß√£o da nota."}), 500
    except Exception as e:
        log.exception(f"Erro ao registrar nota manual: {note[:50]}...")
        return jsonify({"error": "Erro ao gravar nota na mem√≥ria."}), 500

# ---------- HEALTH / PINGNET --------------------------------------- #
@app.route("/pingnet", methods=["GET"])
def pingnet():
    return jsonify({
        "status":   "ok",
        "utc":      datetime.datetime.utcnow().isoformat(timespec="seconds"),
        "host":     socket.gethostname(),
        "model":    OPENAI_MODEL,
        "github_enabled": bool(github_repo_obj),
        "search_enabled": bool(google_search_service),
        "pinecone_enabled": bool(pinecone_index)
    })

# ---------- STATS R√ÅPIDAS ------------------------------------------ #
@app.route("/stats", methods=["GET"])
def stats():
    upload_count = 0
    try:
        if os.path.exists(app.config["UPLOAD_FOLDER"]):
             upload_count = len(os.listdir(app.config["UPLOAD_FOLDER"]))
    except Exception as e:
        log.warning(f"Erro ao listar uploads: {e}")

    pinecone_stats_info = "N/A (Desabilitado ou Erro)"
    if pinecone_index:
        try:
             stats_desc = pinecone_index.describe_index_stats()
             if hasattr(stats_desc, 'total_vector_count'):
                  pinecone_stats_info = f"Vectors: {stats_desc.total_vector_count}"
             elif hasattr(stats_desc, 'namespaces') and 'namespaces' in stats_desc:
                 pinecone_stats_info = f"Namespaces: {len(stats_desc.namespaces)}" # Ou outra info relevante
             else:
                 pinecone_stats_info = "Stats dispon√≠veis, formato inesperado."
        except Exception as e:
            log.exception("Erro ao obter stats do Pinecone.")
            pinecone_stats_info = f"Erro ao obter stats: {type(e).__name__}"

    return jsonify({
        "pinecone_index": PINECONE_INDEX_NAME if pinecone_index else "Desabilitado",
        "pinecone_stats": pinecone_stats_info,
        "uploads_count": upload_count,
        "active_model": OPENAI_MODEL,
        "github_repo": GITHUB_REPO if github_repo_obj else "Desabilitado",
        "search_enabled": bool(google_search_service),
    })

# ---------- UPLOAD ------------------------------------------------- #
@app.route("/upload", methods=["POST"])
def upload_file():
    can_register_memory = bool(pinecone_index)

    if "file" not in request.files:
        log.warning("Tentativa de upload sem arquivo.")
        return jsonify({"message": "Nenhum arquivo anexado!"}), 400

    file = request.files["file"]
    original_filename = file.filename

    if not original_filename:
         log.warning("Tentativa de upload com nome de arquivo vazio.")
         return jsonify({"message": "Nome de arquivo inv√°lido."}), 400

    if not ALLOWED_EXTENSIONS(original_filename):
        allowed_set = IMAGE_EXTS | CODE_EXTS
        log.warning(f"Tentativa de upload de tipo n√£o permitido: {original_filename}")
        return jsonify({"message": f"Arquivo n√£o permitido! Extens√µes v√°lidas: {', '.join(sorted(list(allowed_set)))}"}), 400

    try:
        safe_base_filename = "".join(c for c in original_filename if c.isalnum() or c in ('-', '_', '.'))
        if not safe_base_filename: safe_base_filename = "arquivo_upload"
        filename_to_save = f"{uuid.uuid4().hex}_{safe_base_filename}"
        filepath = os.path.join(app.config["UPLOAD_FOLDER"], filename_to_save)

        file.save(filepath)
        file_size = os.path.getsize(filepath)
        log.info(f"Arquivo recebido: '{original_filename}', salvo como: '{filename_to_save}', Tamanho: {file_size} bytes")

        if can_register_memory:
             try:
                 # <<< MODIFICADO >>> Usar get_text_embedding para o vetor de registro
                 upload_description = f"Upload de arquivo realizado: {original_filename}"
                 embedding_vector_upload = get_text_embedding(upload_description)
                 if embedding_vector_upload:
                     register_memory(
                         pinecone_index, "eli",
                         upload_description,
                         f"Nome no servidor: {filename_to_save}, Tipo: {file.mimetype}, Tamanho: {file_size} bytes",
                         tag="upload"
                     )
                     log.info(f"Evento de upload registrado na mem√≥ria para: {filename_to_save}")
                 else:
                     log.warning(f"N√£o foi poss√≠vel gerar embedding para registrar upload de {filename_to_save}")
             except Exception as reg_upload_e:
                 log.exception(f"Erro ao registrar upload na mem√≥ria para {filename_to_save}")

        return jsonify({"message": "Upload realizado com sucesso!", "filename": filename_to_save})

    except Exception as e:
         log.exception(f"Erro cr√≠tico durante o upload do arquivo: {original_filename}")
         return jsonify({"message": "Erro interno no servidor durante o upload."}), 500

# ---------- SERVIR ARQUIVOS --------------------------------------- #
@app.route("/uploads/<filename>")
def uploaded_file(filename):
    upload_dir = app.config['UPLOAD_FOLDER']
    log.debug(f"Tentando servir arquivo: {filename} de {upload_dir}")
    if ".." in filename or filename.startswith("/"):
         log.warning(f"Tentativa de acesso inv√°lido a arquivo: {filename}")
         return "Acesso inv√°lido.", 400
    try:
        return send_from_directory(upload_dir, filename, as_attachment=False)
    except FileNotFoundError:
         log.warning(f"Arquivo solicitado para download n√£o encontrado: {filename}")
         return "Arquivo n√£o encontrado.", 404
    except Exception as e:
         log.exception(f"Erro ao servir arquivo {filename}")
         return "Erro interno ao servir arquivo.", 500

# ------------------------------------------------------------------ #
#  EXECU√á√ÉO PRINCIPAL                                                #
# ------------------------------------------------------------------ #
if __name__ == "__main__":
    host = os.getenv("HOST", "0.0.0.0")
    port = int(os.getenv("PORT", 8080))
    debug_mode = os.getenv("FLASK_DEBUG", "False").lower() in ['true', '1']

    log.info(f"Iniciando Nash Copilot API v2 (Evolutivo + Ferramentas) em {host}:{port} | Debug Mode: {debug_mode}")
    if debug_mode:
        log.warning("### ATEN√á√ÉO: Rodando em MODO DEBUG. N√ÉO use em produ√ß√£o! ###")
        app.run(host=host, port=port, debug=True)
    else:
        log.info("Modo Produ√ß√£o. Use um servidor WSGI (Gunicorn/Waitress) via Start Command.")
        try:
            import gunicorn
            log.info("Gunicorn detectado. Use 'gunicorn nash_app:app' como Start Command no Railway.")
        except ImportError:
            log.warning("Gunicorn n√£o encontrado. Considere adicion√°-lo aos requirements e us√°-lo no Start Command.")
        # Fallback para o servidor de desenvolvimento Flask (n√£o ideal para produ√ß√£o)
        app.run(host=host, port=port, debug=False)

```

**Observa√ß√µes:**

1.  **`register_memory`:** Notei que na vers√£o anterior, `register_memory` talvez n√£o estivesse recebendo o vetor de embedding explicitamente. Ajustei as chamadas dentro de `/chat`, `/propose_code_change`, `/remember`, e `/upload` para primeiro chamar `get_text_embedding` e depois passar o resultado (ou n√£o registrar se o embedding falhar). **Verifique se a sua fun√ß√£o `register_memory` em `nash_utils.py` est√° preparada para receber o vetor ou se ela mesma gera o embedding internamente.** Se ela gera internamente, voc√™ pode remover as chamadas `get_text_embedding` antes de `register_memory` nestes pontos. A vers√£o que te passei de `nash_utils.py` *n√£o* recebia o vetor, ela gerava internamente, ent√£o voc√™ talvez precise ajustar a chamada `register_memory` ou a pr√≥pria fun√ß√£o em `nash_utils.py` conforme a sua implementa√ß√£o final dela. A vers√£o que te passei *antes* (`nash_utils.py (Completo e Revisado)`) j√° faz o embedding dentro de `register_memory`, ent√£o as chamadas no c√≥digo acima est√£o um pouco redundantes se voc√™ usou *aquela* vers√£o de `nash_utils.py`. Escolha uma abordagem e mantenha a consist√™ncia.
2.  **Caminho Uploads:** Ajustei `UPLOAD_FOLDER` para usar `os.path.join(os.path.dirname(__file__), 'uploads')` que √© um pouco mais robusto para encontrar a pasta relativa ao script em diferentes ambientes.
3.  **Robustez:** Adicionei mais verifica√ß√µes (Pinecone ativo, API Key presente) e logging.

Cole este c√≥digo, fa√ßa o commit e push. Agora o Nash deve usar o prompt din√¢mico *e* ainda ter as capacidades de ferramenta dispon√≠veis! Me diga como foi o teste final!